{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895dcf56-87c8-4dc2-9b66-d6dd5eb3742e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "The implementation of prposed opionion miner is precceeded by the following setup and configuration.\n",
    "-The dataset are placed in the project file.\n",
    "-Insatll all the neccessary libaries and dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca8a98ed-c681-4cff-aced-4cbb29b7bfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.54.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.34.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Using cached torch-2.7.1-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.14.0 torch-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib seaborn nltk spacy transformers scikit-learn torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7815874d-e6d5-44b1-89f1-2971f65e2fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558ab65-665f-4825-836c-63d600d695b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c26b4526-37f0-4f83-a3f1-67ba87d6c196",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46aa0459-38c7-4a2f-ab9f-6cda498eac48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Processing folder: Reviews-9-products\n",
      "📁 Processing folder: CustomerReviews-3_domains\n",
      "📁 Processing folder: Customer_review_data\n",
      "✅ All reviews saved to output/combined_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_review_file(filepath, domain):\n",
    "    data = []\n",
    "    current_sentence = \"\"\n",
    "    encodings = ['utf-8', 'ISO-8859-1', 'cp1252']\n",
    "\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding=enc) as file:\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if not line or line.startswith(\"[t]\") or line.startswith(\"***\"):\n",
    "                        continue\n",
    "\n",
    "                    if line.startswith(\"##\"):\n",
    "                        current_sentence = line[2:].strip()\n",
    "                    else:\n",
    "                        annotations = re.findall(r\"([\\w\\s\\-&]+?)\\[(\\+|\\-)(\\d)\\]\", line)\n",
    "                        for feature, polarity, strength in annotations:\n",
    "                            data.append({\n",
    "                                \"domain\": domain,\n",
    "                                \"sentence\": current_sentence,\n",
    "                                \"feature\": feature.strip(),\n",
    "                                \"sentiment\": \"positive\" if polarity == \"+\" else \"negative\",\n",
    "                                \"strength\": int(strength)\n",
    "                            })\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def parse_all_reviews(root_folder):\n",
    "    all_data = []\n",
    "    output_root = \"output/per_dataset\"\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "    for folder in os.listdir(root_folder):\n",
    "        dataset_path = os.path.join(root_folder, folder)\n",
    "        if not os.path.isdir(dataset_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"📁 Processing folder: {folder}\")\n",
    "        domain_data = []\n",
    "\n",
    "        for file in os.listdir(dataset_path):\n",
    "            if file.endswith(\".txt\"):\n",
    "                path = os.path.join(dataset_path, file)\n",
    "                domain = os.path.splitext(file)[0]\n",
    "                df = parse_review_file(path, domain)\n",
    "                domain_data.append(df)\n",
    "\n",
    "        if domain_data:\n",
    "            df_combined = pd.concat(domain_data, ignore_index=True)\n",
    "            df_combined.to_csv(f\"{output_root}/{folder}.csv\", index=False)\n",
    "            all_data.append(df_combined)\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_reviews = parse_all_reviews(\"data\")\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    df_reviews.to_csv(\"output/combined_reviews.csv\", index=False)\n",
    "    print(\"✅ All reviews saved to output/combined_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5cb722-3330-40a7-bdf8-ef0769e846fa",
   "metadata": {},
   "source": [
    "The above code converts raw .txt to CSV by prasing.\n",
    "The conversion is done and saved in separate CSVs per dataset folder.\n",
    "Also adds 'domain' column for ease of anylsis and coding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a677b-6476-40e9-b67c-eab4aa3575e7",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13561ba1-6b4c-4711-9ce5-5358258e057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mambaforever/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing with stopword removal, aspect normalization, POS tagging and NP extraction...\n",
      "\n",
      "📄 Sample preview:\n",
      "   domain                                           sentence  \\\n",
      "0  norton                                                NaN   \n",
      "1  norton  Why is it that I can install any other type of...   \n",
      "2  norton  Why is it that I can install any other type of...   \n",
      "3  norton  Why is it that I can install any other type of...   \n",
      "4  norton  Why is it that I can install any other type of...   \n",
      "\n",
      "                                  clean_sentence              feature  \\\n",
      "0                                                            software   \n",
      "1  install type software installs works properly      Norton products   \n",
      "2  install type software installs works properly  McAfee Anti-Virus 8   \n",
      "3  install type software installs works properly               Norton   \n",
      "4  install type software installs works properly               Norton   \n",
      "\n",
      "        clean_feature              aspect  \\\n",
      "0            software            software   \n",
      "1     norton products     norton products   \n",
      "2  mcafee antivirus 8  mcafee antivirus 8   \n",
      "3              norton              norton   \n",
      "4              norton              norton   \n",
      "\n",
      "                                            pos_tags  \\\n",
      "0                                                      \n",
      "1  install_VERB type_NOUN software_NOUN installs_...   \n",
      "2  install_VERB type_NOUN software_NOUN installs_...   \n",
      "3  install_VERB type_NOUN software_NOUN installs_...   \n",
      "4  install_VERB type_NOUN software_NOUN installs_...   \n",
      "\n",
      "                       noun_phrases  \n",
      "0                                []  \n",
      "1  [install type software installs]  \n",
      "2  [install type software installs]  \n",
      "3  [install type software installs]  \n",
      "4  [install type software installs]  \n",
      "\n",
      "✅ Cleaned & enriched data saved to output/cleaned_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "ASPECT_SYNONYMS = {\n",
    "    \"battery\": [\"battery\", \"battery life\", \"battery backup\", \"batteries\"],\n",
    "    \"screen\": [\"screen\", \"display\", \"lcd\", \"monitor\"],\n",
    "    \"sound\": [\"sound\", \"audio\", \"speaker\", \"volume\"],\n",
    "    \"camera\": [\"camera\", \"photo\", \"image\", \"pictures\"],\n",
    "    \"price\": [\"price\", \"cost\", \"value\", \"worth\"],\n",
    "    \"performance\": [\"performance\", \"speed\", \"slow\", \"fast\"],\n",
    "    \"memory\": [\"memory\", \"storage\", \"ram\", \"space\"],\n",
    "    \"connectivity\": [\"wifi\", \"bluetooth\", \"connection\", \"connectivity\"],\n",
    "    \"size\": [\"size\", \"weight\", \"dimension\"],\n",
    "    \"design\": [\"design\", \"look\", \"build\", \"style\"]\n",
    "}\n",
    "\n",
    "REVERSE_MAP = {word: aspect for aspect, words in ASPECT_SYNONYMS.items() for word in words}\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        tokens = text.split()\n",
    "        text = \" \".join([t for t in tokens if t not in STOPWORDS])\n",
    "\n",
    "    return text\n",
    "\n",
    "def normalize_feature(feature):\n",
    "    return REVERSE_MAP.get(feature.lower(), feature.lower())\n",
    "\n",
    "def extract_pos_tags(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([f\"{token.text}_{token.pos_}\" for token in doc])\n",
    "\n",
    "def extract_noun_phrases(text):\n",
    "    doc = nlp(text)\n",
    "    return [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "def preprocess_dataframe(df, remove_stopwords=True):\n",
    "    df[\"clean_sentence\"] = df[\"sentence\"].apply(lambda x: clean_text(x, remove_stopwords))\n",
    "    df[\"clean_feature\"] = df[\"feature\"].apply(lambda x: clean_text(x, remove_stopwords=False))\n",
    "    df[\"aspect\"] = df[\"clean_feature\"].apply(normalize_feature)\n",
    "\n",
    "\n",
    "    df[\"pos_tags\"] = df[\"clean_sentence\"].apply(extract_pos_tags)\n",
    "    df[\"noun_phrases\"] = df[\"clean_sentence\"].apply(extract_noun_phrases)\n",
    "\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"output/combined_reviews.csv\")\n",
    "\n",
    "    print(\"✅ Preprocessing with stopword removal, aspect normalization, POS tagging and NP extraction...\")\n",
    "    df_clean = preprocess_dataframe(df, remove_stopwords=True)\n",
    "\n",
    "    df_clean = df_clean.dropna(subset=[\"clean_sentence\", \"clean_feature\", \"aspect\"])\n",
    "\n",
    "    print(\"\\n📄 Sample preview:\")\n",
    "    print(df_clean[[\"domain\", \"sentence\", \"clean_sentence\", \"feature\", \"clean_feature\", \"aspect\", \"pos_tags\", \"noun_phrases\"]].head())\n",
    "\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    df_clean.to_csv(\"output/cleaned_reviews.csv\", index=False)\n",
    "    print(\"\\n✅ Cleaned & enriched data saved to output/cleaned_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ccd65-4500-4bb4-8922-28f109d6165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this sub-task of preprocessing is done. Essential stopwords are downloaded to be used in the spaCy, aspect synonymys are set.\n",
    "POS tagging and noun phrasingis also intialised in this code, these are two key aspects that helps in the next sub-suystem of product feature extraction.\n",
    "Thus effective results of this code is essential nor next sub-tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8f554c-e227-4254-b55b-3b303f7262e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cfe64d3-5626-4e1c-98a1-272de8d3d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dataset shape: (5684, 10)\n",
      "\n",
      "🔍 Sample rows:\n",
      "                                            sentence  \\\n",
      "1  Why is it that I can install any other type of...   \n",
      "2  Why is it that I can install any other type of...   \n",
      "3  Why is it that I can install any other type of...   \n",
      "4  Why is it that I can install any other type of...   \n",
      "5          I bought NIS 2004 recently to try it out.   \n",
      "\n",
      "                                  clean_sentence              feature  \\\n",
      "1  install type software installs works properly      Norton products   \n",
      "2  install type software installs works properly  McAfee Anti-Virus 8   \n",
      "3  install type software installs works properly               Norton   \n",
      "4  install type software installs works properly               Norton   \n",
      "5                   bought nis 2004 recently try         Installation   \n",
      "\n",
      "        clean_feature sentiment  strength  \n",
      "1     norton products  negative         3  \n",
      "2  mcafee antivirus 8  positive         2  \n",
      "3              norton  negative         2  \n",
      "4              norton  negative         2  \n",
      "5        installation  negative         2  \n",
      "\n",
      "🎯 Sentiment distribution:\n",
      "sentiment\n",
      "positive    3832\n",
      "negative    1852\n",
      "Name: count, dtype: int64\n",
      "\n",
      "💥 Sentiment strength distribution:\n",
      "strength\n",
      "2    2306\n",
      "1    1993\n",
      "3    1385\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🏷️ Number of unique features: 1370\n",
      "🏷️ Number of unique aspects: 1345\n",
      "🏷️ Number of unique domains: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/sfw2c31s1pqdjs6bbdhksdqh0000gn/T/ipykernel_55710/2132492848.py:29: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(data=df, x=\"sentiment\", palette=\"Set2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 POS tag frequency (top 15):\n",
      "NOUN     16996\n",
      "VERB      9534\n",
      "ADJ       5734\n",
      "ADV       3099\n",
      "PROPN     2597\n",
      "NUM       1970\n",
      "AUX       1402\n",
      "PART       689\n",
      "PRON       676\n",
      "ADP        525\n",
      "SCONJ      268\n",
      "INTJ       198\n",
      "DET        158\n",
      "CCONJ       59\n",
      "X           39\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🧠 Top Noun Phrases:\n",
      "[]                                                                                   547\n",
      "['phone exculsivly jabra freespeak']                                                  57\n",
      "['pros']                                                                              56\n",
      "['postmark date', 'second bill postmark date']                                        54\n",
      "['cons']                                                                              27\n",
      "['zen micro']                                                                         24\n",
      "['description creatives site', 'actual battery life']                                 23\n",
      "['something']                                                                         20\n",
      "['replaced speaker ebay purchase hope replacement worry']                             19\n",
      "['real industrial designers']                                                         18\n",
      "['good bad things phone', 'long review']                                              18\n",
      "['hand', 'i', 'another one']                                                          18\n",
      "['new business capable phone', 'deal stylus equipped pda', 'device']                  17\n",
      "['greater market', 'sorely terms', 'quality value', 'virtual vacuum competitors']     17\n",
      "['i']                                                                                 17\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/sfw2c31s1pqdjs6bbdhksdqh0000gn/T/ipykernel_55710/2132492848.py:63: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=pos_series.values, y=pos_series.index, palette=\"magma\")\n",
      "/var/folders/x7/sfw2c31s1pqdjs6bbdhksdqh0000gn/T/ipykernel_55710/2132492848.py:78: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=np_series.values, y=np_series.index, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Aspect Mention Count by Domain:\n",
      "domain          Apex AD2600 Progressive-scan DVD player  Canon G3  \\\n",
      "aspect                                                              \n",
      "0                                                     0         0   \n",
      "1800                                                  0         0   \n",
      "2004                                                  0         0   \n",
      "2004 edition                                          0         0   \n",
      "2004 version                                          0         0   \n",
      "3650                                                  0         0   \n",
      "3d                                                    0         0   \n",
      "4mp                                                   0         1   \n",
      "4mp camera                                            0         1   \n",
      "4mp resolution                                        0         1   \n",
      "\n",
      "domain          Canon PowerShot SD500  Canon S100  Computer  \\\n",
      "aspect                                                        \n",
      "0                                   0           0         0   \n",
      "1800                                0           0         0   \n",
      "2004                                0           0         0   \n",
      "2004 edition                        0           0         0   \n",
      "2004 version                        0           0         0   \n",
      "3650                                0           0         0   \n",
      "3d                                  0           0         1   \n",
      "4mp                                 0           0         0   \n",
      "4mp camera                          0           0         0   \n",
      "4mp resolution                      0           0         0   \n",
      "\n",
      "domain          Creative Labs Nomad Jukebox Zen Xtra 40GB  Diaper Champ  \\\n",
      "aspect                                                                    \n",
      "0                                                       3             0   \n",
      "1800                                                    0             0   \n",
      "2004                                                    0             0   \n",
      "2004 edition                                            0             0   \n",
      "2004 version                                            0             0   \n",
      "3650                                                    0             0   \n",
      "3d                                                      0             0   \n",
      "4mp                                                     0             0   \n",
      "4mp camera                                              0             0   \n",
      "4mp resolution                                          0             0   \n",
      "\n",
      "domain          Hitachi router  Linksys Router  MicroMP3  Nikon coolpix 4300  \\\n",
      "aspect                                                                         \n",
      "0                            0               0         0                   0   \n",
      "1800                         0               1         0                   0   \n",
      "2004                         0               0         0                   0   \n",
      "2004 edition                 0               0         0                   0   \n",
      "2004 version                 0               0         0                   0   \n",
      "3650                         0               0         0                   0   \n",
      "3d                           0               0         0                   0   \n",
      "4mp                          0               0         0                   1   \n",
      "4mp camera                   0               0         0                   0   \n",
      "4mp resolution               0               0         0                   0   \n",
      "\n",
      "domain          Nokia 6600  Nokia 6610  Router  Speaker  ipod  norton  \n",
      "aspect                                                                 \n",
      "0                        0           0       0        0     0       0  \n",
      "1800                     0           0       0        0     0       0  \n",
      "2004                     0           0       0        0     0       1  \n",
      "2004 edition             0           0       0        0     0       1  \n",
      "2004 version             0           0       0        0     0       1  \n",
      "3650                     1           0       0        0     0       0  \n",
      "3d                       0           0       0        0     0       0  \n",
      "4mp                      0           0       0        0     0       0  \n",
      "4mp camera               0           0       0        0     0       0  \n",
      "4mp resolution           0           0       0        0     0       0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x7/sfw2c31s1pqdjs6bbdhksdqh0000gn/T/ipykernel_55710/2132492848.py:96: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All EDA visualizations saved to /figures\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import ast\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "\n",
    "def run_eda(df):\n",
    "    print(\"\\U0001F4CA Dataset shape:\", df.shape)\n",
    "    print(\"\\n🔍 Sample rows:\")\n",
    "    print(df[[\"sentence\", \"clean_sentence\", \"feature\", \"clean_feature\", \"sentiment\", \"strength\"]].head())\n",
    "\n",
    "    print(\"\\n🎯 Sentiment distribution:\")\n",
    "    print(df[\"sentiment\"].value_counts())\n",
    "\n",
    "    print(\"\\n💥 Sentiment strength distribution:\")\n",
    "    print(df[\"strength\"].value_counts())\n",
    "\n",
    "    print(\"\\n🏷️ Number of unique features:\", df[\"clean_feature\"].nunique())\n",
    "    print(\"🏷️ Number of unique aspects:\", df[\"aspect\"].nunique())\n",
    "    print(\"🏷️ Number of unique domains:\", df[\"domain\"].nunique() if \"domain\" in df.columns else \"N/A\")\n",
    "\n",
    "    os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "    \n",
    "    sns.countplot(data=df, x=\"sentiment\", palette=\"Set2\")\n",
    "    plt.title(\"Sentiment Distribution\")\n",
    "    plt.xlabel(\"Sentiment\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/sentiment_distribution.png\")\n",
    "    plt.clf()\n",
    "\n",
    "   \n",
    "    sns.countplot(data=df, x=\"strength\", hue=\"sentiment\", palette=\"Set1\")\n",
    "    plt.title(\"Sentiment Strength by Polarity\")\n",
    "    plt.xlabel(\"Strength (1–3)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/sentiment_strength.png\")\n",
    "    plt.clf()\n",
    "\n",
    "   \n",
    "    df[\"sentence_length\"] = df[\"clean_sentence\"].apply(lambda x: len(x.split()))\n",
    "    sns.histplot(df[\"sentence_length\"], bins=30, kde=True)\n",
    "    plt.title(\"Sentence Length Distribution\")\n",
    "    plt.xlabel(\"Number of Words\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/sentence_length_distribution.png\")\n",
    "    plt.clf()\n",
    "\n",
    "  \n",
    "    if \"pos_tags\" in df.columns:\n",
    "        print(\"\\n\\U0001F9E0 POS tag frequency (top 15):\")\n",
    "        pos_flat = [token.split(\"_\")[-1] for row in df[\"pos_tags\"].dropna() for token in row.split()]\n",
    "        pos_series = pd.Series(pos_flat).value_counts().head(15)\n",
    "        print(pos_series)\n",
    "\n",
    "        sns.barplot(x=pos_series.values, y=pos_series.index, palette=\"magma\")\n",
    "        plt.title(\"Top 15 POS Tags\")\n",
    "        plt.xlabel(\"Frequency\")\n",
    "        plt.ylabel(\"POS Tag\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"figures/pos_tag_distribution.png\")\n",
    "        plt.clf()\n",
    "\n",
    "    \n",
    "    if \"noun_phrases\" in df.columns:\n",
    "        print(\"\\n🧠 Top Noun Phrases:\")\n",
    "        all_nps = [np for row in df[\"noun_phrases\"].dropna() for np in row.split(\";\") if np.strip()]\n",
    "        np_series = pd.Series(all_nps).value_counts().head(15)\n",
    "        print(np_series)\n",
    "\n",
    "        sns.barplot(x=np_series.values, y=np_series.index, palette=\"viridis\")\n",
    "        plt.title(\"Top 15 Noun Phrases\")\n",
    "        plt.xlabel(\"Frequency\")\n",
    "        plt.ylabel(\"Noun Phrase\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"figures/noun_phrase_distribution.png\")\n",
    "        plt.clf()\n",
    "\n",
    "   \n",
    "    if \"domain\" in df.columns:\n",
    "        aspect_domain = df.groupby([\"aspect\", \"domain\"]).size().unstack(fill_value=0)\n",
    "        print(\"\\n📊 Aspect Mention Count by Domain:\")\n",
    "        print(aspect_domain.head(10))\n",
    "\n",
    "        aspect_domain.T.plot(kind=\"barh\", stacked=True, colormap=\"tab20\")\n",
    "        plt.title(\"Aspect Distribution per Domain\")\n",
    "        plt.xlabel(\"Count\")\n",
    "        plt.ylabel(\"Domain\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"figures/aspect_per_domain_distribution.png\")\n",
    "        plt.clf()\n",
    "\n",
    "    print(\"\\n✅ All EDA visualizations saved to /figures\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"output/cleaned_reviews.csv\")\n",
    "    df.dropna(subset=[\"clean_sentence\", \"clean_feature\", \"sentiment\", \"strength\"], inplace=True)\n",
    "    run_eda(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2311f-90b7-4f5f-8ecd-05d30c3e05b1",
   "metadata": {},
   "source": [
    "The code visulaise and anlayis through plot few aspects of the so far cleaned data for effective product feature extraction. \n",
    "The plot are used to visulaise Sentiment distribution, Sentiment strength distribution, Distribution of sentence length, Noun Phrase Distribution and Aspect distribution across all domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c34917-c825-4508-8ad0-c7b2cec73b6a",
   "metadata": {},
   "source": [
    "Realtion mapping\n",
    "Mapping of features into recpetive sentiments is done in two methods for the comparive study component in the assignment.\n",
    "-Rule based mapping\n",
    "-Model based mapping(BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c6a6b1-f82e-4b7e-b3fb-d718c7376ac8",
   "metadata": {},
   "source": [
    "Rule based mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ce6134-f720-47e8-9288-aaeaaac45b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/mambaforever/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mambaforever/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Performing rule-based relation mapping...\n",
      "✅ Rule-based relation mapping complete. Saved to output/relation_mapping_rule_based.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download(\"opinion_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "POSITIVE = set(opinion_lexicon.positive())\n",
    "NEGATIVE = set(opinion_lexicon.negative())\n",
    "\n",
    "def find_opinion_word(token):\n",
    "    \n",
    "    for child in token.children:\n",
    "        if child.pos_ == \"ADJ\":\n",
    "            return child.text\n",
    "    if token.head.pos_ == \"ADJ\":\n",
    "        return token.head.text\n",
    "    return None\n",
    "\n",
    "def rule_based_relation_mapping(df):\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row[\"clean_sentence\"]\n",
    "        aspect = row[\"aspect\"]\n",
    "        domain = row[\"domain\"]\n",
    "\n",
    "        if not isinstance(aspect, str) or not isinstance(sentence, str):\n",
    "            continue\n",
    "\n",
    "        doc = nlp(sentence)\n",
    "\n",
    "        for token in doc:\n",
    "            if token.text.lower() == aspect:\n",
    "                opinion = find_opinion_word(token)\n",
    "                if opinion:\n",
    "                    sentiment = (\n",
    "                        \"positive\" if opinion.lower() in POSITIVE else\n",
    "                        \"negative\" if opinion.lower() in NEGATIVE else\n",
    "                        \"neutral\"\n",
    "                    )\n",
    "\n",
    "                    results.append({\n",
    "                        \"domain\": domain,\n",
    "                        \"aspect\": aspect,\n",
    "                        \"clean_sentence\": sentence,\n",
    "                        \"opinion_word\": opinion,\n",
    "                        \"sentiment\": sentiment,\n",
    "                        \"method\": \"rule-based\"\n",
    "                    })\n",
    "                    break \n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"output/cleaned_reviews.csv\"\n",
    "    output_path = \"output/relation_mapping_rule_based.csv\"\n",
    "\n",
    "    df = pd.read_csv(input_path)\n",
    "    df.dropna(subset=[\"clean_sentence\", \"aspect\", \"domain\"], inplace=True)\n",
    "\n",
    "    print(\"🔍 Performing rule-based relation mapping...\")\n",
    "    result_df = rule_based_relation_mapping(df)\n",
    "\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Rule-based relation mapping complete. Saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19f7e4-2ae8-4c4a-9047-33990f914dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "The primary objective of the rule based model is to search for adjectives directly related to the feature token and mapp into it.\n",
    "Certain checkpoints like code to avoid duplicate matches in the same sentence are placed for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34f1aaa9-1357-40a3-ad31-446ac5db2fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.7.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3-Clause\n",
      "Location: /opt/anaconda3/lib/python3.12/site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, setuptools, sympy, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae26358-4b21-4066-acd6-2e8af6f2b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model based mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66a290a6-b46c-43bc-9cb0-4bc213ebd734",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maspect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero-shot-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/bart-large-mnli\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m sentiment_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/__init__.py:998\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    994\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou cannot use both `pipeline(... torch_dtype=..., model_kwargs=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:...})` as those\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    995\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m arguments might conflict, use only one.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m         )\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, torch_dtype):\n\u001b[1;32m    999\u001b[0m         torch_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, torch_dtype)\n\u001b[1;32m   1000\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch_dtype\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"output/cleaned_reviews.csv\")\n",
    "\n",
    "df = df.dropna(subset=[\"clean_sentence\", \"aspect\", \"domain\"])\n",
    "df = df[df[\"clean_sentence\"].str.len() > 5]\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "sentiment_labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "results = []\n",
    "print(\"🔍 Predicting sentiment using BERT (BART MNLI)...\")\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Classifying\"):\n",
    "    sentence = row[\"clean_sentence\"]\n",
    "    aspect = row[\"aspect\"]\n",
    "    domain = row[\"domain\"]\n",
    "\n",
    "    hypothesis_template = f\"The sentiment toward the {aspect} is {{}}\"\n",
    "\n",
    "    try:\n",
    "        prediction = classifier(\n",
    "            sequences=sentence,\n",
    "            candidate_labels=sentiment_labels,\n",
    "            hypothesis_template=hypothesis_template\n",
    "        )\n",
    "        predicted_label = prediction[\"labels\"][0]\n",
    "        confidence = prediction[\"scores\"][0]\n",
    "\n",
    "        results.append({\n",
    "            \"domain\": domain,\n",
    "            \"aspect\": aspect,\n",
    "            \"clean_sentence\": sentence,\n",
    "            \"predicted_sentiment\": predicted_label,\n",
    "            \"confidence\": round(confidence, 3),\n",
    "            \"method\": \"bert-based\"\n",
    "        })\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"✅ Processed {idx} rows\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error on row {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "out_path = \"output/relation_mapping_bert_based.csv\"\n",
    "pd.DataFrame(results).to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ BERT-based relation mapping complete.\")\n",
    "print(f\"📄 Output saved to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ea00e-c6c7-47e0-8ea8-de5c200c9817",
   "metadata": {},
   "source": [
    "The code is contructed to load the cleaned dataset then filter out rows with usable data before load BERT-style classifier(BART).\n",
    "Sentiment classes to predict are defined along adding a progress bar since the code is time complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa75a95-1ce2-4daa-8493-9d620bb4f006",
   "metadata": {},
   "source": [
    "Comparison of rule based mapped data and model based mapped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062937d3-b1c4-49b6-a2ea-7b85dbe09e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "\n",
    "rule_df = pd.read_csv(\"output/relation_mapping_rule_based.csv\")\n",
    "bert_df = pd.read_csv(\"output/relation_mapping_bert_based.csv\")\n",
    "\n",
    "\n",
    "rule_df = rule_df.rename(columns={\n",
    "    \"sentiment\": \"rule_sentiment\",\n",
    "    \"clean_sentence\": \"sentence\"\n",
    "})\n",
    "\n",
    "bert_df = bert_df.rename(columns={\n",
    "    \"predicted_sentiment\": \"bert_sentiment\",\n",
    "    \"clean_sentence\": \"sentence\"\n",
    "})\n",
    "\n",
    "\n",
    "merged = pd.merge(rule_df, bert_df, on=[\"sentence\", \"aspect\"], how=\"inner\")\n",
    "\n",
    "\n",
    "merged[\"agreement\"] = merged[\"rule_sentiment\"] == merged[\"bert_sentiment\"]\n",
    "\n",
    "\n",
    "total = len(merged)\n",
    "agreement = merged[\"agreement\"].sum()\n",
    "agreement_rate = agreement / total if total > 0 else 0\n",
    "\n",
    "print(f\"✅ Compared {total} overlapping sentence-aspect pairs\")\n",
    "print(f\"🤝 Agreement count: {agreement}\")\n",
    "print(f\"📊 Agreement rate: {agreement_rate:.2%}\")\n",
    "\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "merged.to_csv(\"output/comparison_result.csv\", index=False)\n",
    "\n",
    "\n",
    "figure_dir = \"figures/comparison\"\n",
    "os.makedirs(figure_dir, exist_ok=True)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(merged[\"rule_sentiment\"], merged[\"bert_sentiment\"],\n",
    "                      labels=[\"positive\", \"neutral\", \"negative\"])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"positive\", \"neutral\", \"negative\"],\n",
    "            yticklabels=[\"positive\", \"neutral\", \"negative\"])\n",
    "plt.xlabel(\"BERT Sentiment\")\n",
    "plt.ylabel(\"Rule-based Sentiment\")\n",
    "plt.title(\"Confusion Matrix: Rule vs BERT\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{figure_dir}/confusion_matrix.png\")\n",
    "plt.clf()\n",
    "\n",
    "# Agreement count plot\n",
    "sns.countplot(data=merged, x=\"agreement\", palette=\"coolwarm\")\n",
    "plt.title(\"Agreement Between Rule and BERT\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Agreement (True/False)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{figure_dir}/agreement_distribution.png\")\n",
    "plt.clf()\n",
    "\n",
    "print(\"📊 Visuals saved to:\")\n",
    "print(f\"   - {figure_dir}/confusion_matrix.png\")\n",
    "print(f\"   - {figure_dir}/agreement_distribution.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65491523-5acf-45fb-b24f-eacd72311f4d",
   "metadata": {},
   "source": [
    "The difference between the two data is analyised and comapared.\n",
    "A Confusion Matrix and a Agreement count plot is generated to visulaise this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60697366-7799-4b88-b588-10001d41ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dfee23-4a6a-4b16-ab93-9b7d26735db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "\n",
    "method = \"bert\"  # Choose: 'bert' or 'rule'\n",
    "\n",
    "input_paths = {\n",
    "    \"bert\": \"output/relation_mapping_bert_based.csv\",\n",
    "    \"rule\": \"output/relation_mapping_rule_based.csv\"\n",
    "}\n",
    "\n",
    "output_file = f\"output/opinion_summary_{method}.csv\"\n",
    "plot_dir = f\"figures/{method}_based\"\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"📥 Using relation mapping from: {input_paths[method]}\")\n",
    "df = pd.read_csv(input_paths[method])\n",
    "\n",
    "\n",
    "if method == \"bert\":\n",
    "    df = df.rename(columns={\"predicted_sentiment\": \"sentiment\"})\n",
    "\n",
    "df = df.dropna(subset=[\"aspect\", \"sentiment\"])\n",
    "\n",
    "\n",
    "summary = df.groupby([\"aspect\", \"sentiment\"]).size().unstack(fill_value=0)\n",
    "summary[\"total_mentions\"] = summary.sum(axis=1)\n",
    "\n",
    "\n",
    "for label in [\"positive\", \"neutral\", \"negative\"]:\n",
    "    if label not in summary.columns:\n",
    "        summary[label] = 0\n",
    "\n",
    "\n",
    "for label in [\"positive\", \"neutral\", \"negative\"]:\n",
    "    summary[f\"{label}_ratio\"] = summary[label] / summary[\"total_mentions\"]\n",
    "\n",
    "\n",
    "summary.to_csv(output_file)\n",
    "print(f\"✅ Opinion summary saved to: {output_file}\")\n",
    "\n",
    "# ---------- PLOTS ---------- #\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Function to annotate bars\n",
    "def annotate_bars(ax):\n",
    "    for p in ax.patches:\n",
    "        width = p.get_width()\n",
    "        ax.text(width + 0.01, p.get_y() + p.get_height()/2,\n",
    "                f'{width:.2f}' if width < 1 else int(width),\n",
    "                va='center')\n",
    "\n",
    "# Top 10 Positive Aspects\n",
    "plt.figure(figsize=(9, 6))\n",
    "top_positive = summary.sort_values(\"positive_ratio\", ascending=False).head(10)\n",
    "ax = sns.barplot(x=\"positive_ratio\", y=top_positive.index, data=top_positive, palette=\"Greens_r\")\n",
    "annotate_bars(ax)\n",
    "plt.title(f\"Top 10 Positive Aspects ({method.title()}-based)\")\n",
    "plt.xlabel(\"Positive Ratio\")\n",
    "plt.ylabel(\"Aspect\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/top_positive_aspects.png\")\n",
    "plt.clf()\n",
    "\n",
    "# Top 10 Negative Aspects\n",
    "plt.figure(figsize=(9, 6))\n",
    "top_negative = summary.sort_values(\"negative_ratio\", ascending=False).head(10)\n",
    "ax = sns.barplot(x=\"negative_ratio\", y=top_negative.index, data=top_negative, palette=\"Reds_r\")\n",
    "annotate_bars(ax)\n",
    "plt.title(f\"Top 10 Negative Aspects ({method.title()}-based)\")\n",
    "plt.xlabel(\"Negative Ratio\")\n",
    "plt.ylabel(\"Aspect\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/top_negative_aspects.png\")\n",
    "plt.clf()\n",
    "\n",
    "# Top 10 Most Mentioned Aspects\n",
    "plt.figure(figsize=(9, 6))\n",
    "top_mentioned = summary.sort_values(\"total_mentions\", ascending=False).head(10)\n",
    "ax = sns.barplot(x=\"total_mentions\", y=top_mentioned.index, data=top_mentioned, palette=\"Blues_r\")\n",
    "annotate_bars(ax)\n",
    "plt.title(f\"Top 10 Most Mentioned Aspects ({method.title()}-based)\")\n",
    "plt.xlabel(\"Total Mentions\")\n",
    "plt.ylabel(\"Aspect\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/top_mentioned_aspects.png\")\n",
    "plt.clf()\n",
    "\n",
    "# Sentiment Composition (Stacked Bar)\n",
    "plt.figure(figsize=(10, 7))\n",
    "comp = top_mentioned[[\"positive\", \"neutral\", \"negative\"]]\n",
    "comp.plot(kind=\"barh\", stacked=True, colormap=\"viridis\")\n",
    "plt.title(f\"Sentiment Composition for Top Aspects ({method.title()}-based)\")\n",
    "plt.xlabel(\"Mentions\")\n",
    "plt.ylabel(\"Aspect\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/aspect_sentiment_composition.png\")\n",
    "plt.clf()\n",
    "\n",
    "# Radar Chart for Sentiment Ratios\n",
    "radar_data = summary.sort_values(\"total_mentions\", ascending=False).head(6)\n",
    "labels = [\"positive_ratio\", \"neutral_ratio\", \"negative_ratio\"]\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()\n",
    "angles += angles[:1]  # close the loop\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "\n",
    "for idx, row in radar_data.iterrows():\n",
    "    values = [row[label] for label in labels]\n",
    "    values += values[:1]  # repeat first to close\n",
    "    ax.plot(angles, values, label=idx)\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels([label.replace(\"_ratio\", \"\").title() for label in labels])\n",
    "ax.set_yticks([0.25, 0.5, 0.75])\n",
    "ax.set_yticklabels([\"25%\", \"50%\", \"75%\"])\n",
    "ax.set_title(f\"Sentiment Ratios Radar Chart ({method.title()}-based)\")\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.4, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/aspect_sentiment_radar.png\")\n",
    "plt.clf()\n",
    "\n",
    "# Overall Pie Chart\n",
    "overall_counts = df[\"sentiment\"].value_counts()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(overall_counts, labels=overall_counts.index, autopct=\"%.1f%%\", colors=[\"green\", \"grey\", \"red\"])\n",
    "plt.title(f\"Overall Sentiment Distribution ({method.title()}-based)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/overall_sentiment_pie.png\")\n",
    "plt.clf()\n",
    "\n",
    "print(f\"📊 All enhanced plots saved to `{plot_dir}/`:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b691c8-ce8c-456f-a1be-be92ec919b7b",
   "metadata": {},
   "source": [
    "A option to chose either rule maped data or the model mapped data is intialised.\n",
    "The predictected is renamed to sentiment for consistency and filtered for valid rows before aggregation.\n",
    "Sentiment ratios are evaluated and saved to a CSV.\n",
    "Visualisation of desired plots are generated for display of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb341c-f2ba-42b8-b008-d90cddd3ac23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c8996-212d-4db8-a22b-07d6aa9c8baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "7ba8a11d-42b4-48bb-951d-79f56c68f82e",
   "metadata": {},
   "source": [
    "Opinion miner with a comparative experiment component\n",
    "\n",
    "Introduction:\n",
    "The objective of this assignment is to implement a opinion miner that operates on the given data while evaluating design of the very build. \n",
    "In the pervious assignment we have already analysed the dataset and strategised the system design. This report explains implementation of that system design while also acknowledging the subjected changes with respect to design and algorithmic decisions.\n",
    "Implementation:\n",
    "The system design is proposed with multiple sub-tasks that deals with specific processes pipelined for optimal efficiency. The system design as mentioned in assignment one(Figure 1) is followed. \n",
    "This documentation is written in the way specified in the assignment, thus one or multiple sub-tasks in the system design with the specific function is mentioned and explained accordingly. \n",
    "Product feature extraction:\n",
    "On analysis it was evident that the data is diverse in multiple aspects. Thus making essential ingestion and preprocessing essential even before exploratory analysis for product feature extraction. \n",
    "Initial ingestion of data is designed to convert the .txt document in the given data into CSV files. As the data set is diverse with respect to rows and columns and also the vocabulary effective pre-processing techniques are used.\n",
    "Parsing: All files with ‘.txt’ extension is converted into CSVs and store in separate files and also a combined CSV is generated.\n",
    "Cleaning: Removal of lowercase, whitespace, punctuations.\n",
    "Normalisation: Normalisation with respect to aspect synonyms is achieved. \n",
    "Linguistic feature extraction: PoS tagging using spaCy and noun phrases are identified.\n",
    "These methods effectively paved way for the extraction of useful( Table 1) that are also further processed for as desired for next steps.\n",
    "Effective mapping of variants of a feature into one single normalised feature is achieved.Few examples of a normalised final features to the raw feature present in the dataset is also shown.(Table 2)\n",
    "\n",
    "Sentiment analysis :\n",
    "Exploratory data analysis : Patterns in the aspect and sentiments are analysed and visualised for using in relational mapping.\n",
    "The extracted features are mapped into plots using per determined plot libraries available for better understanding of the cleaned and normalised data.\n",
    "Sentiment plots: Two bar chart that shows the overall count of positive and the negative features with respect to strength and distribution across all mentions features is used to identify the sentiment bias.\n",
    "Sentence length plot: A histogram that analyses the number of words per sentence that helps to detect inconsistencies in the cleaned data.\n",
    "Frequency of POS Tag / Noun phrase : Bar charts that lists the most frequently appearing POS tag and noun phrases are shown to help in key grammatical structures and aspect detection respectively.\n",
    "Domain wise sentiment distribution plot : A stacked bar chart that compares the domain level distribution of sentiments(both positive and negative).\n",
    "Sentiment vs Strength heat map: A heat map is used to show the count of sentiment to strength combinations. This particularly helps in understanding the distribution dynamics of sentiments. \n",
    "Now, when proceeding to the relation mapping stage in the proposed system design, the particular sub-task is identified to be optimal for the comparative experiment where two methods are used and compared among them on implementation as specified in the assignment. \n",
    "Hence, the relation mapping to link each product feature mentioned to the sentiment polarity expressed toward that feature is achieved by two approaches. \n",
    "Rule-Based relation mapping:\n",
    "In the basic and simple approach we use linguistic rules and dependency parsing to achieve effective mapping of a sentiment into a extracted feature.\n",
    "Parsing is done with spaCy and search for sentiment is specific to adjectives near that using modifier ADJ. Then checked for negative or positive lexicon before assigning accordingly.\n",
    "BERT-Based relation mapping:\n",
    "Uses a zero-shot classification pre-trained model from Hugging face that maps sentiment to a feature using NLI(Natural Language Interface).\n",
    "A generated hypothesis is tested against sentiments(positive, neutral, negative) to test the score before mapping into the corresponding sentiment of the highest score.\n",
    "Sentiment aggregation: Identifies features of the customers and user priorities to aggregate sentiments per product feature. \n",
    "A option to select from either of the relation mapping model to work on is given at this stage.\n",
    "\n",
    "Evaluation and discussion:\n",
    "The opinion miner that extract feature-level sentiments from reviews. It performs a aspect-based analysis on a multi-domain dataset.\n",
    "Data Ingestion:\n",
    "The proposed method of ingestion is well preserved and well-prepared for cross-domain comparisons. The dataset keeping track of the source folder by adding a domain column is crucial for domain-specific analysis.\n",
    "\n",
    "\n",
    "Preprocessing:\n",
    "Techniques used in cleaning and normalisation is effective and consistent. Rule-based mapping thrives on the linguistic normalisation achieved at this stage.\n",
    "Exploratory data analysis:\n",
    "Visualisation offered is great for interpretability and throws focus insights with respect to linguistic and opinion aspects.\n",
    "Relation-mapping:\n",
    "Since two parallel approaches is used, evaluation and comparing the two methods is used as operate element in code to analysis and visualise the differences between them.\n",
    "Comparative experiment: On analysis on the outputs of both the methods we conclude the following results.\n",
    "Though rule-based method is simple, transparent and requires minimal resources it depends heavily on the grammar of the reviews. While in Bert-based pre trained model is relatively close in identifying the slang grammar in the reviews.\n",
    "Bert-based relation mapping is time consuming and sometimes ambiguous on mapping sentiment as depends on contextual cues.\n",
    "Two plots are generated to show the agreement rate and confusion matrix with respect to the prediction of the two model. \n",
    "Opinion aggregation:\n",
    "Operation on the user selective relation mapping model is easy to operate of varying use preferences. \n",
    "\n",
    "Conclusion:\n",
    "The proposed opinion miner system is well-pipelined combines techniques of optimal data-processing and analysis that allows for both human understandable and model-driven interpretations.\n",
    "\n",
    "\n",
    "Figure 1\n",
    "+--------------------+\n",
    "|  Raw .txt Reviews  |\n",
    "+--------------------+\n",
    "          |\n",
    "          v\n",
    "+------------------------+\n",
    "|  Data Ingestion        |\n",
    "+------------------------+\n",
    "          |\n",
    "          v\n",
    "+------------------------+\n",
    "|  Text Preprocesssing   |\n",
    "+------------------------+\n",
    "          |\n",
    "          v\n",
    "+------------------------------------------+\n",
    "| Sentence Segmentation & Tokenization     |\n",
    "+------------------------------------------+\n",
    "         /                            \\\n",
    "        v                              v\n",
    "+-------------------------+  +-----------------------+\n",
    "| Product Feature         |  | Sentiment             |\n",
    "| Extraction              |  | Extraction            |\n",
    "+-------------------------+  +-----------------------+\n",
    "         \\                            /\n",
    "          v                          v\n",
    "           +-----------------------+\n",
    "           |   Relation Mapping    |\n",
    "           +-----------------------+\n",
    "                     |\n",
    "                     v\n",
    "           +------------------------+\n",
    "           |  Opinion Aggregation   |\n",
    "           +------------------------+\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Table 1 \n",
    "\n",
    "+-------------------------+---------------------------------------------------+-------------------------------------------+\n",
    "| Stage                  | Process                                           | Extracted feature                         |\n",
    "+-------------------------+---------------------------------------------------+-------------------------------------------+\n",
    "| Ingestion and cleaning | Parsing, nltk Stopwords                          | 'feature', 'sentiment', 'strength',       |\n",
    "|                         |                                                   | 'sentence'                                |\n",
    "+-------------------------+---------------------------------------------------+-------------------------------------------+\n",
    "| Normalisation           | ASPECT_SYNONYMS dictionary, reverse mapping      | 'clean_feature', 'battery'                |\n",
    "+-------------------------+---------------------------------------------------+-------------------------------------------+\n",
    "| PoS tagging             | spaCy                                             | POS tags are stored                       |\n",
    "+-------------------------+---------------------------------------------------+-------------------------------------------+\n",
    "| Noun phrase extraction  | spaCy noun_chunks                                 | Noun phrases identified                   |\n",
    "+-------------------------+---------------------------------------------------+-------------------------------------------+\n",
    "\n",
    "\n",
    "Table 2\n",
    "\n",
    "+------------------------+------------------------------------------+\n",
    "| Normalised feature     | Corresponding raw features               |\n",
    "+------------------------+------------------------------------------+\n",
    "| 'battery'              | 'battery life', 'batteries'              |\n",
    "+------------------------+------------------------------------------+\n",
    "| 'screen'               | 'screen', 'display'                      |\n",
    "+------------------------+------------------------------------------+\n",
    "| 'sound'                | 'audio', 'volume'                        |\n",
    "+------------------------+------------------------------------------+\n",
    "| 'design'               | 'look', 'style'                          |\n",
    "+------------------------+------------------------------------------+\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
